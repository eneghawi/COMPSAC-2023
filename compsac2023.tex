\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{soul}
\usepackage{multirow}
\usepackage{lmodern}
\usepackage{framed}
\usepackage{color}
\usepackage{booktabs}
\usepackage{anyfontsize}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\nocite{*}

\title{Linking Team-level and Organization-level Governance in MLOps through XAI and RAI Connector  \\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{xxxxxxxxxxxx}
\IEEEauthorblockA{\textit{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx} \\
\textit{xxxxxxxxxxxxxxxxxxxx}\\
xxxxxxxxxxxxxxxx \\
xxxxxxxxxxxxxxxxxxxxxxxxx}

\IEEEauthorblockN{xxxxxxxxxxxx}
\IEEEauthorblockA{\textit{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx} \\
\textit{xxxxxxxxxxxxxxxxxxxx}\\
xxxxxxxxxxxxxxxx \\
xxxxxxxxxxxxxxxxxxxxxxxxx}

\and

\IEEEauthorblockN{xxxxxxxxxxxx}
\IEEEauthorblockA{\textit{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx} \\
\textit{xxxxxxxxxxxxxxxxxxxx}\\
xxxxxxxxxxxxxxxx \\
xxxxxxxxxxxxxxxxxxxxxxxxx}


\IEEEauthorblockN{xxxxxxxxxxxx}
\IEEEauthorblockA{\textit{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx} \\
\textit{xxxxxxxxxxxxxxxxxxxx}\\
xxxxxxxxxxxxxxxx \\
xxxxxxxxxxxxxxxxxxxxxxxxx}
}

\maketitle

\begin{abstract}
	Adoption of AI systems has been widely used across multiple industry domains at an alerting rate without the focus on it's ethical concerns. To address those concerns, there are an increase number of AI ethics frameworks that have been suggested recently that focus on the algorithmic level rather on the systems level. Nonetheless, some of the system level approaches developed mostly cover a single level governance pattern of the system components in the entire software design lifecycle. However, the need to go beyond the single level  system design AI ethics frameworks to allow not only a better responsible-AI-by-design, but also a trustworthy process patterns that abstract and link the underlying layers of responsible AI on each and every level. This paper illustrates a principal-to-practice guide of the multi-level governance within organizations across the globe for AI ethics frameworks. We outline the main areas of gap in organizations for AI ethics frameworks. Consecutively, we propose a multi-level governance pattern for responsible AI systems within organizations which is participatory, iterative, flexible and operationalizable that target those main gap areas. Finally, to assist practitioners to apply the multi-level governance AI in organizations and the impact that it has on the industry level, we will translate into effective and responsible AI practices using a case study.
\end{abstract}

\begin{IEEEkeywords}
AI, AI ethics, trustworthy AI, XAI, AIMLOps, AIOps, software engineering, software architecture, pattern, best practice
\end{IEEEkeywords}

\section{Introduction}
Aritificial Intelligence (AI) reshaped our lives, helped people make better predictions and take more informed and wise decisions. However, these high tech are still in there infancy, and there remains much promise for AI to promote innovation and address global challenges that people face.

Consecutively, ethical concerns and anxieties are fuelling around AI \cite{DBLP}. There are lots of enquiries on the trustworthiness and adoption of AI systems, including concerns about exacerbating inequality, digital divide, climate change and market concentration. Additionally, there are concerns that the use of AI may compromise human rights and values such as privacy. To address these concerns and ensure the responsible development and use of AI, a collaborative effort involving multiple stakeholders and international cooperation issued guidelines and ethical principles. Despite the creation of ethical guidelines for AI development inside organization, it can be challenging for developers to apply these principles in practical situations. These principles are often abstract and may not provide clear direction for specific implementation \cite{abs-2111-09478}. Therefore, more specific and actionable guidelines are needed to assist developers in implementing ethical considerations in their AI systems. XAI, or Explainable Artificial Intelligence, can provide clear explanations for the decision-making process of AI systems. This understanding can help build trust and confidence in the system and its developers. In addition to transparency and accountability, XAI can also help ensure that AI systems are non-discriminatory. It is important to bridge the gap between ethical principles and the algorithms used in AI systems to ensure responsible development. However, the architecture of an AI ecosystem consists of three layers: AI software supply chain, AI system, and operation infrastructure. It is challenging to show the contribution of each.

One work that was proposed is Responsible AI (RAI) Pattern Catalogue \cite{catalogue}, which takes a pattern-oriented approach to promoting RAI in practice. Instead of solely focusing on ethical principles or AI algorithms, this catalogue focuses on design patterns that practitioners can apply to ensure that their AI systems are responsible throughout the software development process. The catalogue is organized into three categories: 1) governance patterns to establish multi-level governance, 2) process patterns to establish trustworthy development processes, and 3) product patterns to integrate responsible design into AI systems. In addition, it focuses on all aspect of the ecosystem (industry-level, organization-level and team-level) without the planning of the design and the developement tools to support the navigation and utilisation of the RAI pattern catalogue.

\begin{figure*}[htbp!!]
	\centering
	\includegraphics[width=0.8\textwidth]{Organization.png}
	\caption{A Deep Dive into the MLOps Workflow with the XRc Phase: Understanding Key Actor Roles and Responsibilities}
	\label{XAIRAIorg}
\end{figure*}
In this paper, we take a different approach by focusing on the organization-level patterns at the system level rather than just the ethical principles or AI algorithms. This approach aims to integrate responsible design in organizations into final AI products by looking at the MLOps in a bigger picture and the design patterns that reshape the system. This is done with the intention of bridging the gap between the organizational-level and team-level and facilitating navigation using MLOps. We start off by looking at the main two levels of an organization with the addition to the team-level and examine the current available methods \cite{Shneiderman, ShneidermanRespo, Jana, Hussain, roadmap}. Then we make the links on where those methods meets and create the best practices using the multi-level governance patterns at the organization level. The overarching research question that has guided this study is:

% \begin{figure*}[h!]
% 	\centering
% 	\includegraphics[width=4.74in]{governance.jpeg}
% 	\caption{Transition from traditional to the current approach}
% 	\label{ReMixMatch}
% \end{figure*}

\vskip 0.1in
\vskip 0.1in

\noindent\fbox{%
    \parbox{.48\textwidth}{%
     What is the multi-level governance pattern principle-to-practice proposed for responsible AI systems to bridge the gap between team-level and organization-level using MLOps?}%
}

\vskip 0.1in
\vskip 0.1in

The main contributions of this paper are as follows:
\begin{itemize}
\item Find the link between team-level governance patterns with the organization-level patterns.
\item Suggest navigation and utilisation team-level governance patterns with the organization-level patterns.
\item Explore a case study that suits this principle-to-practice multi-level governance pattern.
\end{itemize}
\smallskip

\section{Related Work}
The issue of creating AI that is ethically accountable has garnered a great deal of interest among both industrial and academic communities. To promote ethical AI practices, a multitude of AI ethics principles and guidelines numbering around 100 have been established by various entities including governments, companies, and organizations \cite{GlobalLandscape}. However, these guidelines are often too general and theoretical for individuals involved in the implementation of AI systems to apply in real-world scenarios.

There has been a concerted effort in the field of AI to address the challenges of RAI. One approach that has gained traction is the development of algorithm-level solutions. These solutions are designed to address specific aspects of the numerous high-level AI ethics principles and guidelines that have been established by various entities. By focusing on a subset of the principles, these algorithmic solutions aim to bring concrete and practical approaches to address some of the ethical concerns related to AI. One approach that developers used is by limiting user access and preventing reverse engineering or modifications to the system design. Rather than providing full access to AI systems by running them locally, it is recommended to offer AI services through cloud-based platforms and manage interactions through APIs \cite{TobyAPI}. As an illustration, access to OpenAI's language model GPT-3 is limited to approved users who can only integrate it into their AI systems via API. Another example is Google Vision AI's facial recognition feature, which is limited to a select few celebrities and is only accessible through API. Despite these efforts, there have been instances where the algorithm has been exposed to the outside without proper internal review and verification, leading to potential issues with the responsible use of AI.

However, it's important to note that these algorithm-level solutions are just one part of the larger picture of RAI. Implementing them alone may not be enough to address all the ethical concerns related to AI, as the principles themselves are often complex and multifaceted. It requires a collaborative effort between researchers, developers, policymakers, and other stakeholders (board members, executives, managers) to ensure that AI is developed and used in an ethical and responsible manner.
\section{Methodology}
In order to build up the links of the multi-level governance for RAI systems within organizations, we first evaluated the available methods at the team and organizational level \cite{catalogue} to understand their strengths and limitations, and identified gaps that provided opportunities for improvement. As shown in Figure \ref{XAIRAIorg}, the hierarchy of organization and team-level stakeholders in the industry is depicted on the left side of the illustration, providing a visual representation of the various levels of responsibility and decision making within the industry. The right side of the figure displays the current methods available, which are being utilized to support the operations and processes of the stakeholders.

The illustration provides a comprehensive overview of the stakeholders involved and the methods being utilized, offering insight into the strengths and limitations of the current methods. In addition, the use of XAI and RAI connectors, as shown in the illustration, can further optimize the operation of the current methods and support the efforts of the stakeholders. Utilizing these connectors can provide a more comprehensive and user-friendly experience, leading to improved outcomes and increased success for the organization and its teams.

Futhermore, we evaluated an examination of Machine Learning Operations (MLOps) technologies and tools for each stage of the project pipeline, as well as the roles involved \cite{mlops-without}. In this examination, we identified the weakness of the method being used as the absence of XAI and RAI. The lack of XAI and RAI in the method being used can result in unintended consequences and decreased trust in the system. Therefore, it is important to consider incorporating these elements into any machine learning project to ensure accountability and transparency. To best to our knowledge there is no standard for implementing the multi-level governance pattern for RAI with XAI in MLOps.

XAI and RAI connector (XRc) can play a crucial role in connecting team-level governance to organization-level governance implementation in MLOps. By providing clear and understandable explanations for the decisions made by machine learning models, XAI helps to increase transparency and accountability at the team level. This can be especially important in complex projects involving multiple stakeholders and team members. RAI, on the other hand, helps to ensure that ethical and moral considerations are taken into account throughout the entire MLOps pipeline. This can involve creating policies and guidelines for RAI, as well as conducting risk assessments and impact evaluations. By incorporating RAI into MLOps, organizations can ensure that their use of AI aligns with their values and meets regulatory requirements.

By introducing XRc into the MLOps, organizations can bridge the gap between team-level governance and organization-level governance implementation in MLOps. This helps to ensure that AI systems are used in a responsible and ethical manner, while also providing a clear and transparent explanation of their decision-making process.

\section{Background on MLOps Workflow Stages}
Constructing a machine learning pipeline can be a challenging endeavor. The pipeline is often constructed incrementally with the assistance of tools that have limited integration capabilities. MLOps seeks to streamline this process by automating the pipeline. It serves as a combination of machine learning, data engineering, and DevOps practices, essentially streamlining and accelerating the operationalization of an ML model (including building, testing, and releasing) by incorporating DevOps practices into the process. Determining which stage should be executed by which actor in the MLOps pipeline is not a straightforward task, and often requires multiple iterations to arrive at a suitable solution. However, through the examination of multiple studies, four major stages have been identified. Subsequently, we will outline each component in detail for that particular stage.
\paragraph{Data Management Phase} It can be challenging due to domain-specific limitations \cite{maydanchik2007data} that affect relationships between attributes, accuracy of historical records, and reliability of state transitions \cite{taleb2018big}. Domain experts play a crucial role in ensuring that data models align with project goals and KPIs. They validate potential data and machine learning models to ensure project requirements are met. Some organizations employ Data Stewardship to oversee data quality management and governance, with defined roles such as chief, business, and technical data stewards, as well as a data quality board \cite{mons2018data}.

\paragraph{ML Preparation Phase} This set of functions in the ML preparation stage deals with classic ML preprocessing tasks. Data quality is important and ensured by various roles with the help of data engineers and stewards. Implementing the ML model requires collaboration between data scientists, domain experts, and those responsible for defining the problem within the domain \cite{treveil2020introducing}. In summary, here are the functions in the ML preparation phase:
\begin{itemize}
	\item ML Data Acquisition: The ML pipeline is fed with relevant data based on the prior declared data management plan and selection by the data engineer.
	\item Data Cleaning and Labeling: The input data is cleaned and labeled for ML operations with the help of data scientists and domain experts \cite{taleb2018big}.
	\item Data Versioning: The separation of test, training, and validation data sets is crucial for the success of ML models and is achieved through data versioning.
\end{itemize}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{ML-OPS_with_XAI.png}
	\caption{The XRc to bridge explainable and responsible operations with machine learning lifecycle}
	\label{XRc-phase}
\end{figure*}
\paragraph{ML Training Phase}
The role of data scientists is crucial in the ML Pipeline Phase. They ensure the flexibility, scalability, and proper technology selection of the ML pipeline, while also working to improve model performance. They select the ML pipeline structure, algorithms, and hyperparameters through model versioning and validation, and are the main users in data processing for big data projects. AutoML techniques \cite{automl} and tools support data scientists and domain experts in efficiently selecting the ML pipeline and training the model. The process includes automation of feature preprocessing, model selection, and hyperparameter optimizationa. To sum up, the following functions are performed in the ML preparation phase:
\begin{itemize}
	\item Pipeline Structure Search: The structure of an ML pipeline depends on the type of data (structured or unstructured) and the technique used to solve the problem (supervised, unsupervised, or semi-supervised learning). The specific performance metrics to be used must also be defined based on the specific domain-specific requirements of the problem being solved.
	\item Algorithm and Hyperparameter Selection: The choice of the most suitable ML algorithm for a problem is made by data scientists. The algorithm's performance can be improved by adjusting its hyperparameters, such as the number of layers in a neural network \cite{kisskalt2020streamlining}. However, this process can be time-consuming and complex which addressed by AutoML in \cite{automl}.
	\item Model Versioning: It is a way to keep track of the interdependencies between an ML model, its data, framework, and modeling procedure. It is important for reverting to previous models if there is a problem in production and for deploying the correct version at the right time. Model versioning increases accountability and is an essential component for managing complex ML models.
\end{itemize}
\paragraph{Deployment Phase} The deployment stage is a pivotal moment in the MLOps process. During this phase, software engineers are responsible for incorporating the approved models into the corresponding applications and ensuring the smooth operation of the entire system. To maintain this stability, MLOps engineers must continuously monitor the model, the application as a whole, and the data being used \cite{treveil2020introducing}. Another key player in this phase is the DevOps engineer, who is in charge of constructing, testing, and deploying the functioning system. In general, it is characterized by the following tasks:
\begin{itemize}
	\item Integration of validated models into the relevant applications by software engineers.
	\item Maintenance of the operational stability of the entire system by MLOps engineers through continuous monitoring of the model, application, and data.
	\item Construction, testing, and deployment of the functioning system by DevOps engineers.
\end{itemize}

\section{XAI and RAI Connector(XRc)}
The integration of XRc into the MLOps pipeline may come with added overhead, however, it proves to be a valuable addition to the process as a whole. The addition of XRc not only reduces the risk of failure in RAI, but it also promotes efficiency by allowing for early detection of any problems with implementing organizational-level governance. This helps to avoid duplicating efforts and ensures that the RAI is being developed effectively. As shown in Figure \ref{XRc-phase}, XRc has been inserted between the ML pipeline phase and the ML deployment phase in order to analyze the changes before migrating into the application incorporating the organization level governance into the process. Let's now delve into the sub-phases of XRc.

\subsection{Model Type Identification}
Both dynamic and static identification methods can be used to identify the type of machine learning model, with dynamic methods involving examination of the model's API or performance, and static methods involving examination of the code and architecture used to implement the model.
\paragraph{Static Identification Method} In code, the type of machine learning model can be identified by examining the architecture, algorithms, and libraries used to implement the model. Understanding the architecture of the model, such as the number of hidden layers or the presence of decision trees, can give a good indication of the type of machine learning model used. Additionally, many machine learning libraries provide pre-built models with clear documentation that specify the type of model being used. The documentation for these libraries usually clearly states the type of model being used. 
For example, in the scikit-learn library \cite{pedregosa2011scikit}, the use of `LogisticRegression` class for logistic regression, which is a supervised learning algorithm, or the `KMeans` class for k-means clustering, which is an unsupervised learning algorithm.
\paragraph{Dynamic Identification Method}There are two main ways to dynamically detect the type of machine learning model:

\begin{itemize}
	\item Examining the model's API or functions: This involves looking at the functions that the model exposes, such as the `predict` function, and determining the type of model based on the inputs and outputs of the function.

	\item Examining the model's performance: This involves evaluating the model's performance on a known dataset and determining the type of model based on the performance metrics and results obtained.
\end{itemize}

The choice of dynamic method depends on the specific requirements of the use case. For example, examining the API or functions can be easiest if they are accessible, whereas evaluating the model's performance on a known dataset may be the only option if there is no API or function access.

\subsection{XAI techniques Implementation}
XAI techniques is implemented by AI/MLOps engineers taking into consideration the ML algorithm and the different audiences. It may require a trade-off between technical detail and simplicity, transparency and fairness, and other factors.
\paragraph{Audience Evaluation} While the audiences is mainly at the organization level, AI/MLOps Engineers can categories their audience into two:
\begin{itemize}
\item End users require simple and understandable explanations of the decisions made by a machine learning model. XAI techniques for this audience might include rule-based systems, decision trees, or prototype-based explanations.
\item Regulators require explanations of the decisions made by a machine learning model to ensure compliance with regulations and ethical standards. Model-agnostic techniques like Local Interpretable Model-Agnostic Explanation (LIME) \cite{lime} or SHapley Additive exPlanation (SHAP) \cite{shap} can provide explanations for the predictions made by any machine learning model.
\end{itemize}
\paragraph{ML Algorithm Evaluation} It is not necessary for an AI/MLOps engineer to have a deep understanding of the specific ML algorithm in order to choose an XAI method. However, having a general understanding of the ML algorithm and its strengths and weaknesses can be helpful in choosing an appropriate XAI method. In addition, since some XAI methods can be used and integrated together, AI/MLOps engineer have to consider the broad range of XAI techniques. Those methods can be categorized into the following categories \cite{XAIREF}:
Model-Agnostic Techniques: Model-agnostic XAI techniques are techniques that can be applied to any machine learning model, regardless of the underlying algorithm or architecture. Examples of model-agnostic XAI techniques include LIME and SHAP.
\begin{itemize}
	\item Model-Specific Techniques: Model-specific XAI techniques \cite{XAIREF} are techniques that are designed specifically for a particular type of machine learning model, such as decision trees or neural networks. Examples of model-specific XAI techniques include saliency maps for neural networks and decision trees for decision trees.

	\item Post-Hoc Techniques: Post-hoc XAI techniques \cite{XAIREF} are techniques that are applied to a trained machine learning model after it has been trained, in order to provide explanations for its decisions and actions. Examples of post-hoc XAI techniques include LIME, SHAP, and saliency maps \cite{kadir2001saliency}.

	\item Integrative Techniques: Integrative XAI techniques are techniques that involve integrating XAI into the training process of a machine learning model, so that explanations can be generated as part of the model's normal operation.
\end{itemize}

These categories are not mutually exclusive and some XAI techniques may fall into multiple categories. The choice of XAI technique will depend on the specific requirements of the use case and the type of model being used.

\subsection{Gathering and Filtering} Gathering and filtering XAI methods involves selecting a subset of XAI methods that are appropriate for a specific use case, and then integrating and combining those methods to provide a more comprehensive and effective explanation of the decisions made by a machine learning model. To effectively utilize XAI methods for a specific use case, here are the steps to gather and filter XAI methods: 
\begin{itemize}
	\item Define the requirements: AI/MLOps engineers should start by defining the requirements based on the audience they are trying to target and the type of model being used. This will help to determine which XAI methods are most appropriate for the use case.
	\item Gather XAI methods: Next, gathering a set of XAI methods that are appropriate and generated by the XAI technique Implementation. This may require back and forth to add, remove certain XAI techniques.
	\item Filter XAI methods: Once a set of XAI methods is gathered, filter the methods based on the specific requirements to the use case. Consider factors such as the complexity of the method, the amount of computational resources required, and the amount of technical detail that is appropriate for the audience.
	\item Combine XAI methods: After filtering the XAI methods, MLOps Engineers might consider combining methods to provide a more comprehensive and effective explanation of the decisions made by the machine learning model. They can use techniques such as multi-modal explanations, ensemble explanations, or hybrid explanations to combine XAI methods.
\end{itemize}
% \begin{table*}[htbp]
% \caption{Mapping of XAI Service Architecture Design to XRc MLOps Phase}
% \begin{center}
% \label{Mapping}
% \begin{tabular}{|l|l|l|}
% \hline
%  & & \\
% \textbf{XAI Microservices \& Provenance Ref} & \textbf{Components of XAI Microservices}  & \textbf{XRc Sub-phase Mapping} \\
%  & & \\ \hline
% \multirow{3}{*}{\textbf{User Interface}} & RESTful API & \multirow{3}{*}{Representation and API} \\
%  & Web Portal & \\
%  & Command Line & \\ \hline 
% \multirow{4}{*}{\textbf{Coordination Center}} & \multirow{2}{*}{Microservice Management} & \multirow{4}{*}{Gathering and Filtering} \\
%  & & \\
%  & \multirow{2}{*}{XAI Operation Management} & \\
%  & & \\ \hline 
% \multirow{5}{*}{\textbf{Microservice Layer}} & Data Representation & \multirow{5}{*}{XAI Techniques Implementation} \\
%  & Data Processing & \\
%  & AI Model & \\
%  & XAI Method & \\
%  & Evaluation Metric & \\ \hline
% \multirow{4}{*}{\textbf{Data Persistent Layer}} & Model Explanation Result & \multirow{2}{*}{XAI Techniques Implementation} \\
%  & XAI Evaluation Result & \\ \cline{2-3} 
%  & Machine Learning Data & \multirow{2}{*}{Model Type Identification} \\
%  & Provenance Data & \\ \hline
% \end{tabular}
% \end{center}
% \end{table*}
\subsection{Representation and API}
XAI representation refers to the format in which the explanations generated by XAI techniques are presented to the user. The representation can be in the form of text, visualizations, or other forms of data, and it depends on the specific requirements of the use case and the target audience.

An XAI API, or Application Programming Interface, is a set of protocols and tools for building software applications that provide access to XAI capabilities. An XAI API allows developers to easily integrate XAI techniques into their applications and provide explanations for the decisions made by machine learning models. For example, an XAI API might provide a set of functions that allow developers to generate explanations for specific predictions made by a machine learning model, or to visualize the model's decision-making process. The XAI API might also provide a set of data structures and protocols for representing the explanations generated by XAI techniques, such as text, visualizations, or other forms of data. Here are some ways to make the XAI API interactive:
\begin{itemize}
	\item Visualizations: Use interactive visualizations, such as heatmaps, bar charts, and scatter plots, to help stakeholders understand the explanations generated by the XAI API. This can include interactive visualizations of the model's behavior, such as feature importance, or visualizations of individual predictions, such as decision trees.
	\item User Input: Allow stakeholders to provide input to the XAI API, such as selecting specific predictions to explain, or adjusting parameters of the explanations. This can help stakeholders to better understand the explanations generated by the API, and to tailor the explanations to their specific needs.
	\item Dynamic Explanations: Provide dynamic explanations that change based on user input or other factors, such as the type of model being used, or the specific prediction being explained. This can help stakeholders to better understand the explanations generated by the API, and to see how changes in the model or input data affect the explanations.
	\item Feedback Mechanisms: Provide feedback mechanisms that allow stakeholders to provide feedback on the explanations generated by the XAI API. This can include simple feedback forms, or more complex mechanisms, such as a feedback rating system. This can help organizations to improve the quality of the explanations generated by the API, and to better meet the needs of stakeholders.
\end{itemize}

\begin{figure*}[htbp!!]
	\centering
	\includegraphics[width=1.0\textwidth]{XAI_MT.png}
	\caption{An XAI and RAI Connector (XRc) implementation case study for cloud-based image classification AI model}
	\label{MIICE}
\end{figure*}

In a nutshell, XAI representation refers to the format in which XAI explanations are presented to the user, while an XAI API provides a set of protocols and tools for integrating XAI techniques into software applications and providing explanations for the decisions made by machine learning models. The choice of XAI representation and API will depend on the specific requirements of the use case and the target audience.
\subsection{Decision Process}
Incorporating an XAI API into the decision-making process at the organizational level can help stakeholders make informed decisions about whether to proceed with or stop a ML algorithm based on the transparency and fairness of the model. To make the XAI API interactive, SW Engineers and MLOps can add features that allow stakeholders to interact with the explanations generated by the API.

\section{Case Study Summary}

We elaborate on an XAI service for cloud AI and develop its XRc implementation as the case study to present the effectiveness of the XRc in improving the transparency and fairness of the machine learning model to the non-technical stakeholders (board members, executives, and managers). 
As Fig.~\ref{MIICE} is shown, the entry point and the exit point are ML Pipeline Phase and Deployment Phase in Figure \ref{XRc-phase} which makes it our XRc . Such XRc implementation takes over the pre-trained image classification model on Cloud AI Service and uses XAI techniques to explain and analyze the pre-trained model. It then further exposes its representation and API to enable the stakeholders to decide on the deployment phase. At first, the model is identified as an image classification. Then for the XAI techniques implementation, we select the Grad-CAM XAI method paired with an approximation model to obtain the explanations that approximate the cloud AI model. We further analyze the explanation result to gather and filter the information concerning the stakeholders. Finally, we develop the APIs to represent such information and XAI operations, allowing stakeholders to interact efficiently with those processes.



\section{Conclusion}
The connection between team-level governance patterns and organization-level patterns can be effectively established through the use of MLOps. The XRc phase provides a clear connection between the two levels of governance, allowing for the navigation and utilization of both patterns. The Design Explanation Microservices and Provenance case study serves as a practical example of this principle-to-practice multi-level governance pattern, showcasing how XAI techniques can be effectively implemented in real-world scenarios to support both team and organizational governance goals. By utilizing the XRc phase in MLOps and examining the results of the case study, organizations can gain a better understanding of how to bridge the gap between team-level and organizational-level governance in a way that is both effective and efficient.

Future work in this area could involve exploring new and innovative XAI techniques and methods to further enhance the efficiency and effectiveness of the XRc phase in MLOps. Additionally, continued research into the relationship between team-level and organization-level governance patterns in the context of MLOps will be important in order to fully understand the best practices and strategies for bridging the gap between these two levels of governance.

\bibliographystyle{IEEEtran}
\bibliography{bibfile.bib}

\end{document}
